{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPcmNth0Lkz9"
   },
   "source": [
    "TODO: Small description for notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lhx6rgnN707Q"
   },
   "source": [
    "## <span style=\"color:black\">Mount drive to notebook</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sW_us8hf7dYG",
    "outputId": "ff3f7bea-dc04-44e3-d195-f1a4910afb3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv1aGtHU8Bby"
   },
   "source": [
    "## <span style=\"color:black\">Install needed packages</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UF91Gqe27gj0",
    "outputId": "afe32999-46f1-4c22-a1c0-b720057cd6c0"
   },
   "outputs": [],
   "source": [
    "!pip install tld\n",
    "!pip install pandas==1.0.5\n",
    "\n",
    "import os\n",
    "import bz2\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Packages for url parsing\n",
    "import tld\n",
    "from tld import get_tld\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Packages for NLP methods\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# We need this dataset in order to use the tokenizer\n",
    "nltk.download('punkt')\n",
    "# Also download the list of stopwords to filter out\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Add constants/paths\n",
    "_DATASETS_PATHS = '/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/Quotebank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eht8yDPlLYyJ"
   },
   "source": [
    "## <span style=\"color:Blue\">1) Load Datasets</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hKQxQffMj3D",
    "outputId": "841f65f5-a2db-4278-e1b0-84f72bf272ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/Quotebank/quotes-2015.json.bz2',\n",
       " '/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/Quotebank/quotes-2016.json.bz2',\n",
       " '/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/Quotebank/quotes-2017.json.bz2',\n",
       " '/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/Quotebank/quotes-2018.json.bz2',\n",
       " '/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/Quotebank/quotes-2019.json.bz2',\n",
       " '/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/Quotebank/quotes-2020.json.bz2']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the datasets paths\n",
    "quote_datasets = sorted(glob.glob(_DATASETS_PATHS+'/*.bz2'))\n",
    "quote_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8xvupkZOcc5"
   },
   "source": [
    "### <span style=\"color:red\"><div style=\"text-align: justify\">In order to deal with extremely large data we decided chunking the data and apply all the processing and data wrangling steps on chunks then combine the chunk results (inspired by this [article](https://towardsdatascience.com/3-simple-ways-to-handle-large-data-with-pandas-d9164a3c02c1))</div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmrSOSGhMnNT",
    "outputId": "f992aef1-143b-4f98-e564-3e6c4fa468c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 1000000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Iterate through years dataset then create an iterator to process chunks of 1M rows at a time\n",
    "for dataset in quote_datasets:\n",
    "    df_reader = pd.read_json(dataset, lines=True, compression='bz2', chunksize=500000)\n",
    "    print('Processing Data:', os.path.basename(dataset))\n",
    "    for chunk in df_reader:\n",
    "        chunk = process_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4OoLMqPLYyJ"
   },
   "source": [
    "## <span style=\"color:Blue\">2) Topic Analysis</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Red\"><div style=\"text-align: justify\">After loading the data, we need to extract the targeted quotes from the dataset that will be used in our project. Thus, there are three methods to do so explained below.</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9U0d0D1LYyJ"
   },
   "source": [
    "### <span style=\"color:green\">A- Manual Extraction Method</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\"><div style=\"text-align: justify\">This method is a naive one which means extracting the rows that their quotes include specific keywords. Accordingly, in order to compile a more thorough keywords, we investigated a couple of articles/papers concerning sexual harassement movements such as MeToo such as: [article 1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6751092/)</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\"><div style=\"text-align: justify\"> Then, we decided to filter the dataset from 2015-2020 with one keyword (metoo) and from the resulted quotes we built a word cloud using [WordCloud Library](https://amueller.github.io/word_cloud/) to select the top 20 keywords mentioned in these quotes and pick the relevant ones. Also, we added keywords from this [website](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/2SRSKJ) and this [article](https://journals.sagepub.com/doi/10.1177/1940161220968081) to end up at the end with 15 keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_too_quotes_path = '/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/metoo-extracted-quotes.csv.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through all datasets and extract the quotes containing metoo\n",
    "for dataset in quote_datasets:\n",
    "    df_reader = pd.read_json(dataset, lines=True, compression='bz2', chunksize=500000)\n",
    "    print('Processing Data:', os.path.basename(dataset))\n",
    "    for chunk in df_reader:\n",
    "        chunk = process_chunk(chunk, utils.manual_extraction, me_too_quotes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xg_RT6ugZ0gx"
   },
   "outputs": [],
   "source": [
    "#Load the saved results from previous cell\n",
    "df = pd.read_csv(me_too_quotes_path)\n",
    "df.drop(df[df == 'quotation'].index)\n",
    "df = df.drop(columns='Unnamed: 0')\n",
    "print(f'The number of quotes that contain metoo: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a word cloud from the extracted quotes\n",
    "df = df.quotation.str.replace('metoo', ' ')\n",
    "#Process texts in quotes\n",
    "cleaned_text = df.apply(utils.process_text)\n",
    "#Join the different processed titles together.\n",
    "long_string = ','.join([text for text_list in cleaned_text.values for text in text_list])\n",
    "#Create a WordCloud object\n",
    "wordcloud = WordCloud(width = 800, height = 800, background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "#Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "#Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['movement', 'women', 'victim', 'campaign', 'sex', 'harass', 'assualt',\n",
    "           'rape', 'misconduct', 'metoo', 'timesup', 'abuse', 'workplace', 'right', 'femin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I636A67KfLqz"
   },
   "source": [
    "Add Plots and mention limitations for this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cggOe48LYyK"
   },
   "source": [
    "### <span style=\"color:green\">B- Parsing URLs Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Y6ap0enkk3zb",
    "outputId": "6b45a10d-2c36-47f1-f873-7b120771a07d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'environment-and-nature'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://www.santacruzsentinel.com/environment-and-nature/20180630/dan-haifley-our-ocean-backyard-beach-cleanups-fight-plastic-pollution'\n",
    "parse_title(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdkjTcsbfTh_"
   },
   "source": [
    "Add Plots and mention limitations for this method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iigpmCU2LYyK",
    "tags": []
   },
   "source": [
    "### <span style=\"color:green\">C- NLP-based Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.print_topics(num_words=3)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCRlHtdzLYyK"
   },
   "source": [
    "## <span style=\"color:Blue\">3) Dataset Augmentation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Red\"><div style=\"text-align: justify\">Here, we enrich our dataset with external ones, so we explore each external dataset and apply preliminary analysis on them.</div></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">A- Twitter Dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Purple\"><div style=\"text-align: justify\">The tweets dataset is acquired from [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/2SRSKJ) which contains 32,071,469 #metoo tweets ranging from October 15, 2017 to March 31, 2020. The dataset only contains tweet IDs, thus, we applied for access to twitter api in order to fetch tweets and their metadata from IDs.</div></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter Developer keys here\n",
    "#It is CENSORED\n",
    "consumer_key = 'NTzTHBeXmtuLiurddtRhyvA5x'\n",
    "consumer_key_secret = 'o4AhlY4tnTg4heGFSkttAa2w5CIiHhXaerckfu0sIaLAXpPEvm'\n",
    "access_token = '1456399642103517184-N6FBPIuzHvj5AjUBiIaFDqlcP6kWc6'\n",
    "access_token_secret = 'kOoQy5iyBd2I7ZIlo4ntPtex9lDps8KVTlvRDl3fyhHs7'\n",
    "\n",
    "#Get access to the Twitter API to fetch tweets\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the needed info from the api (Tweet, Location and Date)\n",
    "tweets_data = utils.ids_to_tweets('/content/drive/Shareddrives/ADA LUNATICS 2021/datasets/me_too_tweets/metoo_project_full_dataset_01.txt', api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">B- Traumatic Events Dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:magenta\">1- Query WikiData</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:magenta\">2- Manual Search</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">C- Non-Traumatic Events Dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:magenta\">1- Movie Releases</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:magenta\">2- Events/Speeches</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sBBBa-ZLYyK"
   },
   "source": [
    "### <span style=\"color:Blue\">4) Correlation/Significance Analysis</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LWwZm86Q8cgJ",
    "outputId": "81bd8e9f-6b5c-45f8-a733-78ca59d336f2"
   },
   "outputs": [],
   "source": [
    "with bz2.open(quote_datasets[5], 'rb') as s_file:\n",
    "  # target url\n",
    "  for instance in s_file:\n",
    "    instance = json.loads(instance) # loading a sample\n",
    "    urls = instance['urls'] # extracting list of links\n",
    "    # print(urls)\n",
    "    domains = []\n",
    "    for url in urls:\n",
    "        tld = parse_title(url)\n",
    "        domains.append(tld)\n",
    "        print(tld.path.split('/')[1])\n",
    "  \n",
    "# # displaying the title\n",
    "# print(\"Title of the website is : \")\n",
    "# print (soup.title.get_text())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
